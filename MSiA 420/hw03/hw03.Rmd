---
title: "hw03"
author: "Samuel Swain"
date: "2023-03-03"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Set seed
set.seed(420)
```

```{r common, message=FALSE}
# Libraries
library(readxl)
library(yaImpute)
library(mgcv)

# Cross Validation
CVInd <- function(n,K) { 
  # n is sample size; K is number of parts; 
  # returns K-length list of indices for each part
  m<-floor(n/K) #approximate size of each part
  r<-n-m*K
  I<-sample(n,n) #random reordering of the indices
  Ind<-list() #will be list of indices for all K parts
  length(Ind)<-K
  for (k in 1:K) {
    if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)
    else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
    Ind[[k]] <- I[kpart] #indices for kth part of data
  }
  Ind
}

# Standardize Function
standardize_predictors <- function(data) {
  predictors <- data
  standardized_predictors <- scale(predictors)
  data <- standardized_predictors
  return(data)
}
```

# Problem 1
```{r hw_data, message=FALSE}
# read in the data
df_1 <- read_excel("HW3_data.xls")

# remove the first column
df_1 <- df_1[, -1]

# Get the mean and sd for columns
means <- colMeans(df_1[, -c(1)])
sds <- apply(df_1[, -c(1)], 2, sd)

# take the base-10 logarithm of the "cost" column
df_1$cost <- log10(df_1$cost)

# standardize predictors
X_1 = df_1[, -c(1)]
df_1[, c(2, 3, 4, 5, 6, 7, 8, 9)] = standardize_predictors(X_1)
```

### 1(a)

```{r question_1a}
#### N-fold cross validation
K<-nrow(df_1)  #N-fold CV on each replicate
n.models = 4 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
MSE<-matrix(0,1,n.models)

for (k in 1:K) {
  train<-as.matrix(df_1[-k,-c(1)])
  test<-as.matrix(df_1[k,-c(1)])
  ytrain<-df_1[-k,1]$cost
  K1=6;K2=8;K3=10;K4=12;
  out<-ann(train,test,K1,verbose=F)
  ind<-t(as.matrix(out$knnIndexDist[,1:K1]))
  yhat[k,1]<-apply(ind,1,function(x) mean(ytrain[x]))
  out<-ann(train,test,K2,verbose=F)
  ind<-t(as.matrix(out$knnIndexDist[,1:K2]))
  yhat[k,2]<-apply(ind,1,function(x) mean(ytrain[x]))
  out<-ann(train,test,K3,verbose=F)
  ind<-t(as.matrix(out$knnIndexDist[,1:K3]))
  yhat[k,3]<-apply(ind,1,function(x) mean(ytrain[x]))
  out<-ann(train,test,K4,verbose=F)
  ind<-t(as.matrix(out$knnIndexDist[,1:K4]))
  yhat[k,4]<-apply(ind,1,function(x) mean(ytrain[x]))
} #end of k loop
MSE[1,]=apply(yhat,2,function(x) sum((y-x)^2))/n

MSEAve <- apply(MSE,2,mean) # averaged mean square CV error
MSEsd <- apply(MSE,2,sd) # SD of mean square CV error
r2<-1-MSEAve/var(y) # CV r^2

k_list = c(K1, K2, K3, K4)
best_K = k_list[which.max(r2)]

cat(
  "KNN N-Fold CV K Performances (R Squared):", "\n", "\n", "Best K:", best_K, "\n", "\n",
  "- K =", K1, ":", round(r2[1], 4), "\n", 
  "- K =", K2, ":", round(r2[2], 4), "\n",  
  "- K =", K3, ":", round(r2[3], 4), "\n",  
  "- K =", K4, ":", round(r2[4], 4), "\n"
)
```

#### N-Fold CV with Nearest Neighbors

Pros:

- Lower bias since the model is trained on the entire data set
- No randomness since we are taking one row at a time instead of a random selection of rows
- Will not overestimate test error rate
- Fast model. Computational expense isn't as much of an issue here

Cons:

- Possibly high computational expense 

### 1(b)
```{r question_1b}
Nrep<-50 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 1 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
RMSE<-matrix(0,Nrep,n.models)
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    train<-as.matrix(df_1[-Ind[[k]],-c(1)])
    test<-as.matrix(df_1[Ind[[k]],-c(1)])
    ytrain<-df_1[-Ind[[k]],1]$cost
    K1=best_K; 
    best_knn<-ann(train,test,K1,verbose=F)
    ind<-as.matrix(best_knn$knnIndexDist[,1:K1])
    yhat[Ind[[k]],1]<-apply(ind,1,function(x) mean(ytrain[x]))
  } #end of k loop
  RMSE[j,]=apply(yhat,2,function(x) sqrt(sum((y-x)^2))/n)
} #end of j loop
sd_prediction_error_ave <- apply(RMSE,2,mean) # sd_prediction_error

cat("Nearest Neighbor Prediction Error Standard Deviation (RMSE):", sd_prediction_error_ave[1])
```

### 1(c)
```{r question_1c}
# create a dataframe for the new observation
new_obs <- data.frame(
  age = 59,
  gend = 0,
  intvn = 10,
  drugs = 0,
  ervis = 3,
  comp = 0,
  comorb = 4,
  dur = 300
)

# standardize the new observation using the mean and SD from the training data
new_obs_std <- scale(new_obs, center = means, scale = sds)[1,]

# Predict on unseen data
train<-as.matrix(df_1[,-c(1)])
test<-t(as.matrix(new_obs_std))
ytrain<-df_1$cost
best_knn<-ann(train,test,best_K,verbose=F)
ind<-t(as.matrix(best_knn$knnIndexDist[,1:best_K]))
D<-as.matrix(best_knn$knnIndexDist[,(1+best_K):(2*best_K)])
prediction<-apply(ind,1,function(x) mean(ytrain[x]))

cat("Cost prediction:", 10^prediction)
```

# Problem 2

### 2(a)
```{r question_2a}
# Fit the GAM model
gam_fit <- gam(cost ~ s(age) + gend + s(intvn) + drugs + s(ervis) + comp + s(comorb) + s(dur), 
               data = df_1, family = gaussian(), sp = c(-1,-1,-1,-1,-1,-1,-1,-1))

# Create a 2x3 grid of plots
par(mfrow = c(2, 3))

# Plot the GAM model
plot(gam_fit, main = "Component Plot")
```

As we can see in the component plots above, intvn by far has the greatest effect on cost. This is consistent with what we've seen in homework 2. We can see that comorb and dur have a slight positive effect on cost at relatively low values but level off quickly.

### 2(b)
```{r question_2b}
##Now use multiple reps of CV to compare Neural Nets and linear reg models###
Nrep<-50 # number of replicates of CV
K<-10 # K-fold CV on each replicate
n.models = 1 # number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models)
RMSE<-matrix(0,Nrep,n.models)
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    out <- gam(cost ~ s(age) + gend + s(intvn) + drugs + s(ervis) + comp + s(comorb) + s(dur), 
               data = df_1[-Ind[[k]],], family = gaussian(), sp = c(-1,-1,-1,-1,-1,-1,-1,-1))
    yhat[Ind[[k]],1] <- as.numeric(predict(out,df_1[Ind[[k]],]))
  } #end of k loop
  RMSE[j,]=apply(yhat,2,function(x) sqrt(sum((y-x)^2))/n)
} #end of j loop

sd_prediction_error_ave <- apply(RMSE,2,mean) # sd_prediction_error

cat("GAM Prediction Error Standard Deviation (RMSE):", sd_prediction_error_ave[1])
```

#### N-Fold CV with Generalized Additive Models

Pros:

- Lower bias since the model is trained on the entire data set
- No randomness since we are taking one row at a time instead of a random selection of rows
- Will not overestimate test error rate

Cons:

- Heavy computational expense. This model is harder to fit than a nearest neighbor model which will increase the run-time by a lot.

### 2(c)
```{r question_2c}
# Predicting using GAM on new data
prediction = predict(gam_fit, newdata = data.frame(t(as.matrix(new_obs_std))))

# Print prediction
cat("Cost prediction:", 10^prediction)
```

# Problem 3

### 3(a)
```{r question_3a, warning=FALSE}
####CV to choose the best K
Nrep<-5 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 1 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
MSE<-matrix(0,Nrep,n.models)

# Hyperparameters to try
span = seq(.02,.2,.02)
degree = c(0, 1, 2)

# Store best params
best_r2 <- -Inf
best_loess_coefs_cv = c(0, 0)

for (dg in degree) {
  for (sp in span) {
    for (j in 1:Nrep) {
      Ind<-CVInd(n,K)
      for (k in 1:K) {
        
        out<-loess(cost ~., df_1[-Ind[[k]], c(1, 4, 8, 9, 6)], degree=dg, span=sp, control=loess.control(surface="direct"))
        yhat[Ind[[k]],1]<-as.numeric(predict(out,df_1[Ind[[k]],]))
        
      } #end of k loop
      MSE[j,]=apply(yhat,2,function(x) sum((y-x)^2))/n
    } #end of j loop
    MSEAve <- apply(MSE,2,mean) # averaged mean square CV error
    MSEsd <- apply(MSE,2,sd) # SD of mean square CV error
    r2<-1-MSEAve/var(y) # CV r^2
    
    if (r2 > best_r2) {
      best_r2 <- r2
      best_loess_coefs_cv <- c(dg, sp)
    }
    
  }
}

cat("Best Loess terms (CV):", "\n", 
    "- degree:", best_loess_coefs_cv[1], "\n",
    "- span:", best_loess_coefs_cv[2], "\n")
```

### 3(b)
```{r question_3b, warning=FALSE}
# Define initial best sigma-hat value
sig_hat_selection <- c(degree = 0, span = 0, s = Inf)

# Loop over degree and span values
for (dg in degree) {
  for (sp in span) {
    # Fit loess model and check s statistic
    out <- loess(cost ~ ., df_1[, c(1, 4, 8, 9, 6)], degree = dg, span = sp)
    
    # Update sigma-hat selection if current s is lower
    if (out$s < sig_hat_selection[3]) {
      sig_hat_selection <- c(degree = dg, span = sp, s = out$s)
    }
  }
}

# Select sigma-hat value
sig_hat <- 0.1

# Print selected sigma-hat values
# print(sig_hat_selection) # Uncomment to see selected s value

# Define initial best parameters
best_loess_coefs_cp <- c(degree = 0, span = 0, Cp = Inf)

# Loop over degree and span values
for (dg in degree) {
  for (sp in span) {
    
    # Fit loess model and compute Cp statistic
    out <- loess(cost ~ ., df_1[, c(1, 4, 8, 9, 6)], degree = dg, span = sp)
    SSE <- sum((df_1$cost - out$fitted)^2)
    Cp <- (SSE + 2 * out$trace.hat * sig_hat^2) / nrow(df_1)

    # Update best parameters if current Cp is lower
    if (Cp < best_loess_coefs_cp[3]) {
      best_loess_coefs_cp <- c(degree = dg, span = sp, Cp = Cp)
    }
  }   
}

# Print best parameters
cat("Best Loess terms:\n",
    "- degree:", best_loess_coefs_cp[1], "\n",
    "- span:", best_loess_coefs_cp[2])
```

Cross validation-chosen hyperparameters are somewhat in agreement with the hyperparameters chosen for Cp. The value Cp chose for span is a little smaller but that's all.

### 3(c)
```{r question_3c, warning=FALSE}
####CV to Find Best nterms in PPR on Concrete Data
Nrep<-50 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 1 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
RMSE<-matrix(0,Nrep,n.models)

for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    out <- loess(cost ~ ., df_1[-Ind[[k]], c(1, 4, 8, 9, 6)], degree = best_loess_coefs_cv[1], span = best_loess_coefs_cv[2], control=loess.control(surface="direct"))
    yhat[Ind[[k]],1]<-as.numeric(predict(out,df_1[Ind[[k]],]))
  } #end of k loop
  RMSE[j,]=apply(yhat,2,function(x) sqrt(sum((y-x)^2))/n)
} #end of j loop
sd_prediction_error_ave <- apply(RMSE,2,mean) # sd_prediction_error

cat("PPR Prediction Error Standard Deviation (RMSE):", sd_prediction_error_ave[1], "\n")
```

### 3(d)
```{r question_3d, warning=FALSE}
# Fit best loess model
best_loess <- loess(cost ~ ., df_1[, c(1, 4, 8, 9, 6)], degree = best_loess_coefs_cv[1], span = best_loess_coefs_cv[2])

# Predicting using PPR on new data
prediction = predict(best_loess, newdata = data.frame(t(as.matrix(new_obs_std))))

# Print prediction
cat("Cost prediction:", 10^prediction)
```

# Question 4

### 4(a)
```{r question_4a}
# Define constants
Nrep <- 5 # Number of replicates of CV
K <- 10 # K-fold CV on each replicate
n.models <- 1 # Number of different models to fit
n <- nrow(df_1)
y <- df_1$cost
yhat <- matrix(0, n, n.models)
MSE <- matrix(0, Nrep, n.models)

# Hyperparameters to try
n_terms <- seq(1, 10, 1)

# Store best params
best_r2 <- -Inf
best_nterms <- 0

# Iterate over hyperparameters
for (term in n_terms) {
  
  # Run cross-validation
  for (j in 1:Nrep) {
    Ind <- CVInd(n, K)
    for (k in 1:K) {
      out <- ppr(cost ~ ., data = df_1[-Ind[[k]], ], nterms = term)  
      yhat[Ind[[k]], 1] <- as.numeric(predict(out, df_1[Ind[[k]], ]))
    } # End of k loop
    MSE[j, ] <- apply(yhat, 2, function(x) sum((y - x)^2)) / n
  } # End of j loop
  
  # Calculate CV r^2 and update best params
  MSEAve <- apply(MSE, 2, mean) # Averaged mean square CV error
  r2 <- 1 - MSEAve / var(y) # CV r^2
  
  if (r2 > best_r2) {
    best_r2 <- r2[1]
    best_nterms <- term
  }
  
} # End of term loop

cat("Best PPR terms:", "\n", "- n_terms:", best_nterms)
```

### 4(b)
```{r question_4b}
# CV to Find Best nterms in PPR on Concrete Data
Nrep <- 50  # number of replicates of CV
K <- 10  # K-fold CV on each replicate
n.models <- 1  # number of different models to fit
n <- nrow(df_1)
y <- df_1$cost
yhat <- matrix(0, n, n.models)
RMSE <- matrix(0, Nrep, n.models)

for (j in 1:Nrep) {
  Ind <- CVInd(n, K)
  for (k in 1:K) {
    out <- ppr(cost ~ ., data = df_1[-Ind[[k]], ], nterms = best_nterms)  
    yhat[Ind[[k]], 1] <- as.numeric(predict(out, df_1[Ind[[k]], ]))
  }
  RMSE[j, ] <- apply(yhat, 2, function(x) sqrt(sum((y - x)^2)) / n)
}

# Calculate prediction error standard deviation (RMSE)
sd_prediction_error_ave <- apply(RMSE, 2, mean)

cat("PPR Prediction Error Standard Deviation (RMSE):", sd_prediction_error_ave[1], "\n\n")

# Final model
best_ppr <- ppr(cost ~ ., data = df_1, nterms = best_nterms)

plot(best_ppr)
summary(best_ppr)
```

I'm not sure how to interpret this.

### 4(c)
```{r question_4c}
# Predicting using PPR on new data
prediction = predict(best_ppr, newdata = data.frame(t(as.matrix(new_obs_std))))

# Print prediction
cat("Cost prediction:", 10^prediction)
```

# Problem 5

```{r data_5, message=FALSE}
# Read data
df_5 <- read_excel("HW3_data.xls", sheet = "FGL data")

# remove the first column
df_5 <- df_5[, -1]

# Convert data to binary classification
df_5$type <- ifelse(df_5$type %in% c("Veh", "WinF", "WinNF"), 0, 1)

# standardize predictors
X_5 = df_5[, -c(10)]
df_5[, -c(10)] = standardize_predictors(X_5)
```

### 5(a)
```{r question_5a}
Nrep<-50 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 1 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
accuracy<-matrix(0,Nrep,n.models)

# Hyperparameters to try
nearest_neighbors = seq(1, 10, 1)

for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    train<-as.matrix(df_1[-Ind[[k]],-c(1)])
    test<-as.matrix(df_1[Ind[[k]],-c(1)])
    ytrain<-df_1[-Ind[[k]],1]$cost
    K1=best_K; 
    best_knn<-ann(train,test,K1,verbose=F)
    ind<-as.matrix(best_knn$knnIndexDist[,1:K1])
    yhat[Ind[[k]],1]<-apply(ind,1,function(x) mean(ytrain[x]))
  } #end of k loop
  accuracy[j,]=apply(yhat,2,function(x) mean(y == x))
} #end of j loop
sd_prediction_error_ave <- apply(accuracy,2,mean) # sd_prediction_error

cat("Nearest Neighbor Prediction Error Standard Deviation (RMSE):", sd_prediction_error_ave[1])
```

### 5(b)
```{r question_5b}

```

### 5(c)
```{r question_5c}

```

# Problem 6

### 6(a)
```{r question_6a}

```

### 6(b)
```{r question_6b}

```

### 6(c)
```{r question_6c}

```

# Problem 7

### 7(a)
```{r question_7a}

```

### 7(b)
```{r question_7b}

```

### 7(c)
```{r question_7c}

```

### 7(d)
```{r question_7d}

```

### 7(e)
```{r question_7e}

```

### 7(f)
```{r question_7f}

```

### 7(g)
```{r question_7g}

```

# Appendix
### Commented out code is the output used to attain the results above. Commended to shorted final submission pdf.