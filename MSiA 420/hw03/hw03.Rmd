---
title: "hw03"
author: "Samuel Swain"
date: "2023-03-03"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Set seed
set.seed(420)
```

```{r common, message=FALSE}
# Libraries
library(readxl)
library(yaImpute)

# Cross Validation
CVInd <- function(n,K) { 
  # n is sample size; K is number of parts; 
  # returns K-length list of indices for each part
  m<-floor(n/K) #approximate size of each part
  r<-n-m*K
  I<-sample(n,n) #random reordering of the indices
  Ind<-list() #will be list of indices for all K parts
  length(Ind)<-K
  for (k in 1:K) {
    if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)
    else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
    Ind[[k]] <- I[kpart] #indices for kth part of data
  }
  Ind
}

# Standardize Function
standardize_predictors <- function(data) {
  predictors <- data
  standardized_predictors <- scale(predictors)
  data <- standardized_predictors
  return(data)
}
```

# Problem 1
```{r hw_data, message=FALSE}
# read in the data
df_1 <- read_excel("HW3_data.xls")

# remove the first column
df_1 <- df_1[, -1]

# take the base-10 logarithm of the "cost" column
df_1$cost <- log10(df_1$cost)

# standardize predictors
X_1 = df_1[, -c(1)]
df_1[, c(2, 3, 4, 5, 6, 7, 8, 9)] = standardize_predictors(X_1)
```

### 1(a)
```{r question_1a}
####CV to choose the best K
Nrep<-50 #number of replicates of CV
K<-10  #K-fold CV on each replicate
n.models = 4 #number of different models to fit
n=nrow(df_1)
y<-df_1$cost
yhat=matrix(0,n,n.models) 
MSE<-matrix(0,Nrep,n.models)
for (j in 1:Nrep) {
  Ind<-CVInd(n,K)
  for (k in 1:K) {
    train<-as.matrix(df_1[-Ind[[k]],-c(1)])
    test<-as.matrix(df_1[Ind[[k]],-c(1)])
    ytrain<-df_1[-Ind[[k]],1]$cost
    K1=5;K2=10;K3=15;K4=20; 
    out<-ann(train,test,K1,verbose=F)
    ind<-as.matrix(out$knnIndexDist[,1:K1])
    yhat[Ind[[k]],1]<-apply(ind,1,function(x) mean(ytrain[x]))
    out<-ann(train,test,K2,verbose=F)
    ind<-as.matrix(out$knnIndexDist[,1:K2])
    yhat[Ind[[k]],2]<-apply(ind,1,function(x) mean(ytrain[x]))
    out<-ann(train,test,K3,verbose=F)
    ind<-as.matrix(out$knnIndexDist[,1:K3])
    yhat[Ind[[k]],3]<-apply(ind,1,function(x) mean(ytrain[x]))
    out<-ann(train,test,K4,verbose=F)
    ind<-as.matrix(out$knnIndexDist[,1:K4])
    yhat[Ind[[k]],4]<-apply(ind,1,function(x) mean(ytrain[x]))
  } #end of k loop
  MSE[j,]=apply(yhat,2,function(x) sum((y-x)^2))/n
} #end of j loop
MSEAve <- apply(MSE,2,mean); MSEAve #averaged mean square CV error
MSEsd <- apply(MSE,2,sd); MSEsd   #SD of mean square CV error
r2<-1-MSEAve/var(y); r2  #CV r^2
plot(yhat[,2],y)
```

### 1(b)
```{r question_1b}

```

### 1(c)
```{r question_1c}

```

# Problem 2

### 2(a)
```{r question_2a}

```

### 2(b)
```{r question_2b}

```

### 2(c)
```{r question_2c}

```

# Problem 3

### 3(a)
```{r question_3a}

```

### 3(b)
```{r question_3b}

```

### 3(c)
```{r question_3c}

```

### 3(d)
```{r question_3d}

```

# Question 4

### 4(a)
```{r question_4a}

```

### 4(b)
```{r question_4b}

```

### 4(c)
```{r question_4c}

```

# Problem 5

### 5(a)
```{r question_5a}

```

### 5(b)
```{r question_5b}

```

### 5(c)
```{r question_5c}

```

# Problem 6

### 6(a)
```{r question_6a}

```

### 6(b)
```{r question_6b}

```

### 6(c)
```{r question_6c}

```

# Problem 7

### 7(a)
```{r question_7a}

```

### 7(b)
```{r question_7b}

```

### 7(c)
```{r question_7c}

```

### 7(d)
```{r question_7d}

```

### 7(e)
```{r question_7e}

```

### 7(f)
```{r question_7f}

```

### 7(g)
```{r question_7g}

```